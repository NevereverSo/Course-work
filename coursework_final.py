import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (silhouette_score, r2_score, mean_absolute_error, 
                           mean_absolute_percentage_error, mean_squared_error,
                           accuracy_score, classification_report)
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing

st.set_page_config(
    page_title="Weather Analytics Dashboard", 
    layout="wide"
)

@st.cache_data(ttl=3600, show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
def load_data():
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        daily_df = pd.read_csv("daily_weather_smallest.csv", low_memory=False)
        if not daily_df.empty and 'date' in daily_df.columns:
            daily_df['date'] = pd.to_datetime(daily_df['date'], errors='coerce')
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ daily_weather_smallest.csv: {str(e)}")
        daily_df = pd.DataFrame()
    
    try:
        cities_df = pd.read_csv("cities.csv", low_memory=False)
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ cities.csv: {str(e)}")
        cities_df = pd.DataFrame()
    
    try:
        countries_df = pd.read_csv("countries.csv", low_memory=False)
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ countries.csv: {str(e)}")
        countries_df = pd.DataFrame()
    
    return daily_df, cities_df, countries_df

@st.cache_data(ttl=1800, max_entries=5)
def prepare_time_series_data(df, target_col, date_col='date'):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    """
    if df.empty or target_col not in df.columns or date_col not in df.columns:
        return None
    
    ts_df = df.sort_values(date_col).drop_duplicates(subset=[date_col])
    
    if len(ts_df) < 10:
        return None
    
    ts_data = ts_df[[date_col, target_col]].copy()
    ts_data.columns = ['ds', 'y']
    
    ts_data['y'] = ts_data['y'].interpolate(method='linear')
    
    zero_threshold_vars = ['precipitation', 'snow', 'depth', 'rain', 'snow_depth']
    if any(var in target_col.lower() for var in zero_threshold_vars):
        ts_data['y'] = ts_data['y'] + np.random.uniform(0.01, 0.1, len(ts_data))
    
    return ts_data

@st.cache_data(ttl=1800, max_entries=3)
def arima_forecast(ts_data, periods=30, order=(1,1,1)):
    """
    –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ ARIMA
    """
    try:
        ts_series = ts_data.set_index('ds')['y']
        
        max_points = 50
        if len(ts_series) > max_points:
            ts_series = ts_series.iloc[-max_points:]
        
        model = ARIMA(ts_series, order=order)
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            try:
                model_fit = model.fit(method_kwargs={'maxiter': 30})
            except:
                st.info("ARIMA (1,1,1) –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º (1,0,0)")
                model = ARIMA(ts_series, order=(1,0,0))
                model_fit = model.fit(method_kwargs={'maxiter': 20})
        
        forecast = model_fit.forecast(steps=periods)
        last_date = ts_series.index[-1]
        forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=periods, freq='D')
        
        forecast_df = pd.DataFrame({
            'ds': forecast_dates,
            'yhat': forecast.values
        })
        
        return model_fit, forecast_df
    except Exception as e:
        st.error(f"ARIMA –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {str(e)[:100]}")
        return None, None

@st.cache_data(ttl=1800, max_entries=3)
def exponential_smoothing_forecast(ts_data, periods=30):
    """
    –ü–†–û–°–¢–ê–Ø –ò –†–ê–ë–û–ß–ê–Ø –≤–µ—Ä—Å–∏—è Exponential Smoothing
    """
    try:
        ts_series = ts_data.set_index('ds')['y']
        
        if len(ts_series) < 5:
            st.warning("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Exponential Smoothing")
            return None, None
        
        max_points = 60
        if len(ts_series) > max_points:
            ts_series = ts_series.iloc[-max_points:]
        
        if len(ts_series) >= 14:
            seasonal_periods = 7
        else:
            seasonal_periods = None
        
        try:
            if seasonal_periods and len(ts_series) >= 2 * seasonal_periods:
                model = ExponentialSmoothing(
                    ts_series,
                    seasonal_periods=seasonal_periods,
                    trend='add',
                    seasonal='add',
                    initialization_method='estimated'
                )
            else:
                model = ExponentialSmoothing(
                    ts_series,
                    seasonal=None,
                    trend='add',
                    initialization_method='estimated'
                )
            
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                try:
                    model_fit = model.fit(optimized=True)
                except:
                    # –ï—Å–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    model_fit = model.fit(
                        smoothing_level=0.3,
                        smoothing_trend=0.1,
                        smoothing_seasonal=0.1 if seasonal_periods else None,
                        optimized=False
                    )
            
            forecast = model_fit.forecast(steps=periods)
            
            if np.any(np.isnan(forecast)):
                st.warning("–ü—Ä–æ–≥–Ω–æ–∑ —Å–æ–¥–µ—Ä–∂–∏—Ç NaN, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥–Ω–æ–∑")
                raise ValueError("NaN in forecast")
            
        except Exception as e:
            st.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å ETS")
            model = ExponentialSmoothing(
                ts_series,
                seasonal=None,
                trend=None,
                initialization_method='estimated'
            )
            model_fit = model.fit()
            forecast = model_fit.forecast(steps=periods)
        
        last_date = ts_series.index[-1]
        forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=periods, freq='D')
        
        forecast_df = pd.DataFrame({
            'ds': forecast_dates,
            'yhat': forecast.values
        })
        
        return model_fit, forecast_df
        
    except Exception as e:
        st.error(f"Exponential Smoothing –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {str(e)[:100]}")
        return None, None

def simple_forecast_fallback(ts_data, periods=30):
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç
    """
    try:
        ts_series = ts_data.set_index('ds')['y']
        
        if len(ts_series) < 2:
            forecast_values = np.zeros(periods)
        else:
            last_value = ts_series.iloc[-1]
            
            if len(ts_series) >= 5:
                recent = ts_series.iloc[-5:].values
                if len(recent) >= 2:
                    x = np.arange(len(recent))
                    coeffs = np.polyfit(x, recent, 1)
                    trend = coeffs[0]
                    
                    forecast_values = last_value + trend * np.arange(1, periods + 1)
                else:
                    forecast_values = np.full(periods, last_value)
            else:
                forecast_values = np.full(periods, last_value)
        
        last_date = ts_data['ds'].iloc[-1]
        forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=periods, freq='D')
        
        return pd.DataFrame({
            'ds': forecast_dates,
            'yhat': forecast_values
        })
    except:
        return pd.DataFrame({
            'ds': pd.date_range(start='2023-01-01', periods=periods, freq='D'),
            'yhat': np.zeros(periods)
        })

def evaluate_time_series_model(ts_data, model_type='arima', test_size=0.3):
    if ts_data is None or len(ts_data) < 30:
        return None, None
    
    split_idx = int(len(ts_data) * (1 - test_size))
    train_data = ts_data.iloc[:split_idx].copy()
    test_data = ts_data.iloc[split_idx:].copy()
    
    if model_type == 'arima':
        _, forecast = arima_forecast(train_data, periods=len(test_data))
    elif model_type == 'exponential':
        _, forecast = exponential_smoothing_forecast(train_data, periods=len(test_data))
    else:
        return None, None
    
    if forecast is None:
        return None, None
    
    y_true = test_data['y'].values
    y_pred = forecast['yhat'].values[:len(y_true)]
    
    min_len = min(len(y_true), len(y_pred))
    y_true = y_true[:min_len]
    y_pred = y_pred[:min_len]
    
    if len(y_true) < 3:
        return None, None
    
    metrics = {}
    
    if len(y_true) > 1:
        naive_forecast = np.zeros_like(y_true)
        naive_forecast[1:] = y_true[:-1]
        naive_forecast[0] = y_true[0]
        
        mae_naive = np.mean(np.abs(y_true[1:] - naive_forecast[1:]))
        mae_model = np.mean(np.abs(y_true - y_pred))
        
        if mae_naive > 0:
            metrics['MASE'] = mae_model / mae_naive
            metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = ((mae_naive - mae_model) / mae_naive) * 100
        else:
            metrics['MASE'] = np.nan
            metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = np.nan
    else:
        metrics['MASE'] = np.nan
        metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = np.nan
    
    if 'MASE' in metrics and not np.isnan(metrics['MASE']):
        if metrics['MASE'] < 1:
            metrics['R¬≤'] = 1 - metrics['MASE']
        else:
            metrics['R¬≤'] = -(metrics['MASE'] - 1)
    else:
        metrics['R¬≤'] = np.nan
    
    metrics['MAE'] = np.mean(np.abs(y_true - y_pred))
    metrics['RMSE'] = np.sqrt(np.mean((y_true - y_pred) ** 2))
    
    mask = y_true != 0
    if np.sum(mask) > 0:
        metrics['MAPE (%)'] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        metrics['MAPE (%)'] = np.nan
    
    return metrics, (y_true, y_pred, test_data['ds'].values)
  
def calculate_metrics(y_true, y_pred, variable_name=""):
    import numpy as np
    
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    if len(y_true) < 3:
        return {'MASE': np.nan, '–£–ª—É—á—à–µ–Ω–∏–µ (%)': np.nan, 'MAE': np.nan, 
                'RMSE': np.nan, 'sMAPE (%)': np.nan, 'R¬≤': np.nan}
    
    metrics = {}
    
    if len(y_true) > 1:
        naive = np.zeros_like(y_true)
        naive[1:] = y_true[:-1]
        naive[0] = y_true[0]
        
        mae_model = np.mean(np.abs(y_true - y_pred))
        mae_naive = np.mean(np.abs(y_true[1:] - naive[1:]))
        
        if mae_naive > 0:
            mase = mae_model / mae_naive
            metrics['MASE'] = float(mase)
            metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = float((1 - mase) * 100)
        else:
            metrics['MASE'] = np.nan
            metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = np.nan
    else:
        metrics['MASE'] = np.nan
        metrics['–£–ª—É—á—à–µ–Ω–∏–µ (%)'] = np.nan
    
    metrics['MAE'] = float(np.mean(np.abs(y_true - y_pred)))
    metrics['RMSE'] = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
    
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    denominator[denominator == 0] = 0.001
    smape = 100 * np.mean(np.abs(y_pred - y_true) / denominator)
    metrics['sMAPE (%)'] = float(min(smape, 200))
    
    if 'MASE' in metrics and metrics['MASE'] is not np.nan:
        if metrics['MASE'] < 1:
            metrics['R¬≤'] = float(1 - metrics['MASE'])
        else:
            metrics['R¬≤'] = float(-(metrics['MASE'] - 1))
    else:
        metrics['R¬≤'] = np.nan
    
    return metrics

daily_df = pd.DataFrame()
cities_df = pd.DataFrame()
countries_df = pd.DataFrame()

try:
    daily_df, cities_df, countries_df = load_data()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
    daily_df = pd.DataFrame()
    cities_df = pd.DataFrame()
    countries_df = pd.DataFrame()

def get_available_cities(df):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤"""
    if df.empty or 'city_name' not in df.columns:
        return []
    cities = sorted(df['city_name'].dropna().unique().tolist())
    return ["–í—Å–µ –≥–æ—Ä–æ–¥–∞"] + cities

def filter_data_by_city(df, selected_city):
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –≥–æ—Ä–æ–¥—É"""
    if df.empty or not selected_city or selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
        return df.copy()
    
    return df[df['city_name'] == selected_city].copy()

def get_city_stats(df, city):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥–æ—Ä–æ–¥—É"""
    if df.empty or city not in df['city_name'].values:
        return {}
    
    city_data = df[df['city_name'] == city]
    
    stats = {
        'total_days': len(city_data),
        'date_range': f"{city_data['date'].min().date()} - {city_data['date'].max().date()}",
        'avg_temp': round(city_data['avg_temp_c'].mean(), 2) if 'avg_temp_c' in city_data.columns else None,
        'avg_precipitation': round(city_data['precipitation_mm'].mean(), 2) if 'precipitation_mm' in city_data.columns else None,
        'avg_pressure': round(city_data['avg_sea_level_pres_hpa'].mean(), 2) if 'avg_sea_level_pres_hpa' in city_data.columns else None,
        'seasons': city_data['season'].unique().tolist() if 'season' in city_data.columns else []
    }
    
    return stats

@st.cache_data
def get_numeric_columns(df):
    """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
    if df.empty:
        return []
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    id_keywords = ['station_id', 'id', 'station', '_id']
    filtered_cols = [col for col in numeric_cols 
                     if not any(keyword in col.lower() for keyword in id_keywords)]
    
    return filtered_cols

@st.cache_data
def prepare_scaled_data(_df, numeric_cols):
    """–ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    if len(_df) == 0 or len(numeric_cols) == 0:
        return pd.DataFrame()
    
    scaler = StandardScaler()
    
    sample_size = min(3000, len(_df))
    if len(_df) > sample_size:
        df_sample = _df.sample(sample_size, random_state=42)
        scaler.fit(df_sample[numeric_cols])
    else:
        scaler.fit(_df[numeric_cols])
    
    df_scaled = _df.copy()
    df_scaled[numeric_cols] = scaler.transform(_df[numeric_cols])
    
    return df_scaled

@st.cache_data
def prepare_classification_data(df, target_col, features):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    if df.empty or target_col not in df.columns:
        return None, None, None, None
    
    median_val = df[target_col].median()
    y = (df[target_col] > median_val).astype(int)
    
    X = df[features].copy()
    
    X = X.fillna(X.mean())
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, scaler, median_val
  
def convert_dates_to_numeric(dates):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—ã –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π —Å –ø–µ—Ä–≤–æ–π –¥–∞—Ç—ã)"""
    if len(dates) == 0:
        return dates
    
    if np.issubdtype(dates.dtype, np.number):
        return dates
    
    if pd.api.types.is_datetime64_any_dtype(dates):
        min_date = dates.min()
        numeric_dates = (dates - min_date).dt.days
        return numeric_dates
    elif hasattr(dates.iloc[0], 'date'):
        min_date = min(dates)
        numeric_dates = [(date - min_date).days for date in dates]
        return pd.Series(numeric_dates)
    
    return dates
  
st.sidebar.title("Weather Analytics")

page = st.sidebar.radio(
    "–ù–∞–≤–∏–≥–∞—Ü–∏—è",
    ["–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", "–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ"]
)

st.sidebar.subheader("–í—ã–±–æ—Ä –≥–æ—Ä–æ–¥–∞")

if 'daily_df' in locals() and not daily_df.empty and 'city_name' in daily_df.columns:
    available_cities = get_available_cities(daily_df)
    
    if available_cities:
        selected_city = st.sidebar.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –≥–æ—Ä–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
            options=available_cities,
            index=0,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –≥–æ—Ä–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. '–í—Å–µ –≥–æ—Ä–æ–¥–∞' –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."
        )
        
        filtered_df = filter_data_by_city(daily_df, selected_city)
        
        if selected_city != "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
            city_stats = get_city_stats(daily_df, selected_city)
            if city_stats:
                st.sidebar.info(f"""
                **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ {selected_city}:**
                - –î–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö: {city_stats['total_days']}
                - –ü–µ—Ä–∏–æ–¥: {city_stats['date_range']}
                - –°—Ä. —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {city_stats['avg_temp']}¬∞C
                - –°—Ä. –æ—Å–∞–¥–∫–∏: {city_stats['avg_precipitation']} –º–º
                - –°—Ä. –¥–∞–≤–ª–µ–Ω–∏–µ: {city_stats['avg_pressure']} –≥–ü–∞
                """)
        
        numeric_cols = get_numeric_columns(filtered_df)
        if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
            st.sidebar.success(f"–í—Å–µ –≥–æ—Ä–æ–¥–∞: {len(filtered_df):,} –∑–∞–ø–∏—Å–µ–π, {len(filtered_df['city_name'].unique())} –≥–æ—Ä–æ–¥–æ–≤")
        else:
            st.sidebar.success(f"{selected_city}: {len(filtered_df):,} –∑–∞–ø–∏—Å–µ–π, {len(numeric_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    else:
        st.sidebar.warning("–ö–æ–ª–æ–Ω–∫–∞ 'city_name' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
        filtered_df = daily_df
        selected_city = "–í—Å–µ –≥–æ—Ä–æ–¥–∞"
        numeric_cols = get_numeric_columns(filtered_df)
else:
    st.sidebar.error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å –≥–æ—Ä–æ–¥–∞–º–∏")
    filtered_df = daily_df if 'daily_df' in locals() else pd.DataFrame()
    selected_city = "–í—Å–µ –≥–æ—Ä–æ–¥–∞"
    numeric_cols = []

# ... (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")

if page == "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö":
    
    if filtered_df.empty:
        st.error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ daily_weather_smallest.csv")
    else:
        if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
            st.header(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: –í—Å–µ –≥–æ—Ä–æ–¥–∞")
        else:
            st.header(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: {selected_city}")
        
        numeric_cols = get_numeric_columns(filtered_df)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if 'city_name' in filtered_df.columns:
                if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
                    unique_cities = filtered_df['city_name'].nunique()
                    st.metric("–ì–æ—Ä–æ–¥–æ–≤", unique_cities)
                else:
                    st.metric("–í—ã–±—Ä–∞–Ω –≥–æ—Ä–æ–¥", selected_city)
            else:
                st.metric("–ó–∞–ø–∏—Å–µ–π", len(filtered_df))
        
        with col2:
            if 'date' in filtered_df.columns:
                unique_days = len(filtered_df['date'].dt.date.unique())
                st.metric("–î–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö", unique_days)
        
        with col3:
            st.metric("–ü—Ä–∏–∑–Ω–∞–∫–æ–≤", len(numeric_cols))
            
        with col4:
            if len(numeric_cols) > 0:
                data_accuracy = filtered_df[numeric_cols].notna().mean().mean() * 100
                st.metric("–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö", f"{data_accuracy:.1f}%")
            else:
                st.metric("–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö", "N/A")
        
        # –ù–û–í–ê–Ø –°–ï–ö–¶–ò–Ø: –ú–µ—Ç–æ–¥—ã info() –∏ describe()
        with st.expander("üìä –ú–µ—Ç–æ–¥—ã info() –∏ describe() –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("–ú–µ—Ç–æ–¥ info()")
                st.write("**–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:**")
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
                info_text = f"""
                **–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:** {filtered_df.shape[0]} —Å—Ç—Ä–æ–∫ √ó {filtered_df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤
                
                **–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:**
                """
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö
                dtypes_info = filtered_df.dtypes.value_counts()
                for dtype, count in dtypes_info.items():
                    info_text += f"\n- {dtype}: {count} –∫–æ–ª–æ–Ω–æ–∫"
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–ø—É—Å–∫–∞—Ö
                missing_info = filtered_df.isnull().sum()
                total_missing = missing_info.sum()
                info_text += f"\n\n**–ü—Ä–æ–ø—É—Å–∫–∏:** {total_missing:,} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"
                
                st.info(info_text)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
                if st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫"):
                    dtype_df = pd.DataFrame({
                        '–ö–æ–ª–æ–Ω–∫–∞': filtered_df.columns,
                        '–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö': filtered_df.dtypes.values,
                        '–ù–µ-null –∑–Ω–∞—á–µ–Ω–∏—è': filtered_df.notna().sum().values,
                        '–ü—Ä–æ–ø—É—Å–∫–∏': filtered_df.isnull().sum().values
                    })
                    st.dataframe(dtype_df, use_container_width=True)
            
            with col2:
                st.subheader("–ú–µ—Ç–æ–¥ describe()")
                
                # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è describe
                if len(numeric_cols) > 0:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    display_cols = numeric_cols[:8]  # –ü–µ—Ä–≤—ã–µ 8 —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    
                    describe_df = filtered_df[display_cols].describe().round(2)
                    
                    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–∏–π
                    describe_df.index = ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–°—Ä–µ–¥–Ω–µ–µ', '–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ', 
                                       '–ú–∏–Ω–∏–º—É–º', '25%', '50% (–º–µ–¥–∏–∞–Ω–∞)', '75%', '–ú–∞–∫—Å–∏–º—É–º']
                    
                    st.dataframe(describe_df, use_container_width=True)
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    if st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"):
                        st.write("**–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏:**")
                        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                        if len(display_cols) > 1:
                            corr_matrix = filtered_df[display_cols].corr().round(3)
                            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                            fig = px.imshow(
                                corr_matrix,
                                text_auto=True,
                                aspect="auto",
                                title="–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏",
                                color_continuous_scale='RdBu_r',
                                range_color=[-1, 1]
                            )
                            st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –º–µ—Ç–æ–¥–∞ describe()")
        
        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if st.button("üì• –≠–∫—Å–ø–æ—Ä—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ CSV"):
            if len(numeric_cols) > 0:
                # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
                stats_list = []
                for col in numeric_cols[:15]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫
                    if col in filtered_df.columns:
                        stats = filtered_df[col].describe()
                        stats_list.append({
                            '–ü—Ä–∏–∑–Ω–∞–∫': col,
                            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': stats['count'],
                            '–°—Ä–µ–¥–Ω–µ–µ': round(stats['mean'], 2),
                            '–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ': round(stats['std'], 2),
                            '–ú–∏–Ω–∏–º—É–º': round(stats['min'], 2),
                            '25%': round(stats['25%'], 2),
                            '–ú–µ–¥–∏–∞–Ω–∞': round(stats['50%'], 2),
                            '75%': round(stats['75%'], 2),
                            '–ú–∞–∫—Å–∏–º—É–º': round(stats['max'], 2),
                            '–ü—Ä–æ–ø—É—Å–∫–∏': filtered_df[col].isnull().sum(),
                            '–¢–æ—á–Ω–æ—Å—Ç—å (%)': round((filtered_df[col].notna().sum() / len(filtered_df)) * 100, 1)
                        })
                
                stats_df = pd.DataFrame(stats_list)
                
                # –°–æ–∑–¥–∞–µ–º CSV
                csv = stats_df.to_csv(index=False, encoding='utf-8-sig')
                
                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                st.download_button(
                    label="–°–∫–∞—á–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É CSV",
                    data=csv,
                    file_name=f"weather_stats_{selected_city}.csv",
                    mime="text/csv",
                )
        
        if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞" and 'city_name' in filtered_df.columns and filtered_df['city_name'].nunique() > 1:
            with st.expander("–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ—Ä–æ–¥–∞–º"):
                city_stats_summary = []
                for city in filtered_df['city_name'].unique():
                    city_data = filtered_df[filtered_df['city_name'] == city]
                    stats = {
                        '–ì–æ—Ä–æ–¥': city,
                        '–î–Ω–µ–π': len(city_data),
                        '–°—Ä. —Ç–µ–º–ø. (¬∞C)': round(city_data['avg_temp_c'].mean(), 1) if 'avg_temp_c' in city_data.columns else 'N/A',
                        '–°—Ä. –æ—Å–∞–¥–∫–∏ (–º–º)': round(city_data['precipitation_mm'].mean(), 1) if 'precipitation_mm' in city_data.columns else 'N/A',
                        '–°—Ä. –¥–∞–≤–ª–µ–Ω–∏–µ (–≥–ü–∞)': round(city_data['avg_sea_level_pres_hpa'].mean(), 1) if 'avg_sea_level_pres_hpa' in city_data.columns else 'N/A',
                        '–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö (%)': round(city_data[numeric_cols].notna().mean().mean() * 100, 1) if len(numeric_cols) > 0 else 'N/A',
                        '–ü–µ—Ä–∏–æ–¥': f"{city_data['date'].min().date()} - {city_data['date'].max().date()}"
                    }
                    city_stats_summary.append(stats)
                
                stats_df = pd.DataFrame(city_stats_summary)
                st.dataframe(stats_df, use_container_width=True)
        
        # –ò–ó–ú–ï–ù–Ø–ï–ú –í–ö–õ–ê–î–ö–ò - –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –≤–∫–ª–∞–¥–∫—É –¥–ª—è info/describe
        tab1, tab2, tab3, tab4 = st.tabs(["–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑", "Scatter Plot", "Box & Violin Plots", "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"])
        
        with tab1:
            st.subheader("–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            if numeric_cols:
                selected_col = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:", 
                    numeric_cols[:15]
                )
                
                if selected_col in filtered_df.columns:
                    data = filtered_df[selected_col]
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("–°—Ä–µ–¥–Ω–µ–µ", f"{data.mean():.2f}")
                    with col2:
                        st.metric("–ú–µ–¥–∏–∞–Ω–∞", f"{data.median():.2f}")
                    with col3:
                        st.metric("–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", f"{data.std():.2f}")
                      
                    with col4:
                        accuracy_pct = data.notna().sum() / len(data) * 100
                        st.metric("–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö", f"{accuracy_pct:.1f}%")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
                    with st.expander("üìà –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞"):
                        col_stats = filtered_df[selected_col].describe()
                        stats_df = pd.DataFrame({
                            '–ú–µ—Ç—Ä–∏–∫–∞': ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–°—Ä–µ–¥–Ω–µ–µ', '–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ', '–ú–∏–Ω–∏–º—É–º', '25%', '50% (–º–µ–¥–∏–∞–Ω–∞)', '75%', '–ú–∞–∫—Å–∏–º—É–º'],
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': [col_stats['count'], 
                                       round(col_stats['mean'], 4),
                                       round(col_stats['std'], 4),
                                       round(col_stats['min'], 4),
                                       round(col_stats['25%'], 4),
                                       round(col_stats['50%'], 4),
                                       round(col_stats['75%'], 4),
                                       round(col_stats['max'], 4)]
                        })
                        st.dataframe(stats_df, use_container_width=True)
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–ø—É—Å–∫–∞—Ö
                        missing_count = filtered_df[selected_col].isnull().sum()
                        if missing_count > 0:
                            st.warning(f"‚ö†Ô∏è –í –∫–æ–ª–æ–Ω–∫–µ '{selected_col}' {missing_count} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({round(missing_count/len(filtered_df)*100, 1)}%)")
                    
                    fig = px.histogram(
                            filtered_df, 
                            x=selected_col, 
                            nbins=30,
                            title=f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {selected_col} - {selected_city}",
                            color='city_name' if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞" and 'city_name' in filtered_df.columns else None
                    )
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            # ... (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è Scatter Plot –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        
        with tab3:
            # ... (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è Box & Violin Plots –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
            
        # –ù–û–í–ê–Ø –í–ö–õ–ê–î–ö–ê –î–õ–Ø –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô
        with tab4:
            st.subheader("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            if numeric_cols:
                # –í—ã–±–æ—Ä –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                selected_features = st.multiselect(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π:",
                    numeric_cols[:10],
                    default=numeric_cols[:3] if len(numeric_cols) >= 3 else numeric_cols
                )
                
                if selected_features:
                    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
                    fig = go.Figure()
                    
                    colors = px.colors.qualitative.Set1
                    
                    for i, feature in enumerate(selected_features):
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                        data = filtered_df[feature].dropna()
                        if len(data) > 0:
                            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
                            fig.add_trace(go.Histogram(
                                x=data,
                                name=feature,
                                opacity=0.6,
                                marker_color=colors[i % len(colors)],
                                nbinsx=30
                            ))
                    
                    fig.update_layout(
                        title=f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {selected_city}",
                        xaxis_title="–ó–Ω–∞—á–µ–Ω–∏—è",
                        yaxis_title="–ß–∞—Å—Ç–æ—Ç–∞",
                        barmode='overlay',
                        bargap=0.1
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    st.subheader("–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                    summary_stats = []
                    for feature in selected_features:
                        if feature in filtered_df.columns:
                            data = filtered_df[feature]
                            stats = data.describe()
                            summary_stats.append({
                                '–ü—Ä–∏–∑–Ω–∞–∫': feature,
                                '–°—Ä–µ–¥–Ω–µ–µ': round(stats['mean'], 3),
                                '–ú–µ–¥–∏–∞–Ω–∞': round(stats['50%'], 3),
                                '–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ': round(stats['std'], 3),
                                '–ú–∏–Ω–∏–º—É–º': round(stats['min'], 3),
                                '–ú–∞–∫—Å–∏–º—É–º': round(stats['max'], 3),
                                '–ü—Ä–æ–ø—É—Å–∫–∏': data.isnull().sum(),
                                '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π': data.nunique()
                            })
                    
                    if summary_stats:
                        summary_df = pd.DataFrame(summary_stats)
                        st.dataframe(summary_df, use_container_width=True)

elif page == "–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö":
    
    if filtered_df.empty:
        st.error("–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ.")
    else:
        if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
            st.header(f"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö: –í—Å–µ –≥–æ—Ä–æ–¥–∞")
        else:
            st.header(f"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö: {selected_city}")
        
        numeric_cols = get_numeric_columns(filtered_df)
        
        if not numeric_cols:
            st.error("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        else:
            st.write(f"**–î–æ—Å—Ç—É–ø–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:** {len(numeric_cols)}")
            
            analysis_method = st.selectbox(
                "–ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:",
                ["–†–µ–≥—Ä–µ—Å—Å–∏—è", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", "PCA"],
                index=0
            )
            
            if analysis_method in ["–†–µ–≥—Ä–µ—Å—Å–∏—è", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", "PCA"]:
                df_scaled = prepare_scaled_data(filtered_df, numeric_cols)
            
            if analysis_method == "–†–µ–≥—Ä–µ—Å—Å–∏—è":
                st.header("–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                
                target = st.selectbox(
                    "–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (Y):", 
                    numeric_cols[:10]
                )
                
                if target:
                    if len(numeric_cols) > 1:
                        correlations = filtered_df[numeric_cols].corr()[target].abs().sort_values(ascending=False)
                        correlations = correlations[correlations.index != target]
                        top_features = correlations.head(3).index.tolist()
                    else:
                        top_features = []
                    
                    features = st.multiselect(
                        "–ü—Ä–∏–∑–Ω–∞–∫–∏ (X):",
                        numeric_cols,
                        default=top_features
                    )
                
                if target and features:
                    test_size = st.slider("–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:", 0.1, 0.8, 0.2, 0.05)
                    
                    if target in filtered_df.columns and pd.api.types.is_datetime64_any_dtype(filtered_df[target]):
                        X = df_scaled[features]
                        y = convert_dates_to_numeric(filtered_df[target])
                        y_scaled = (y - y.mean()) / y.std()
                    else:
                        X = df_scaled[features]
                        y = df_scaled[target]
                    
                    sample_size = min(2000, len(X))
                    if len(X) > sample_size:
                        sample_idx = np.random.choice(len(X), sample_size, replace=False)
                        X_sample = X.iloc[sample_idx]
                        y_sample = y.iloc[sample_idx] if hasattr(y, 'iloc') else y[sample_idx]
                        X_train, X_test, y_train, y_test = train_test_split(
                            X_sample, y_sample, test_size=test_size, random_state=42
                        )
                    else:
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=test_size, random_state=42
                        )
                    
                    models_config = {
                        "–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è": LinearRegression(),
                        "–ì—Ä–µ–±–Ω–µ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è": Ridge(alpha=1.0),
                        "–õ–∞—Å—Å–æ —Ä–µ–≥—Ä–µ—Å—Å–∏—è": Lasso(alpha=0.01)
                    }
                    
                    results = {}
                    
                    for name, model in models_config.items():
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        
                        metrics = calculate_metrics(y_test, y_pred, target)
                        
                        results[name] = {
                            'R¬≤': metrics['R¬≤'],
                            'MAE': metrics['MAE'],
                            'RMSE': metrics['RMSE'],
                            'MAPE (%)': metrics.get('MAPE (%)', 'N/A'),
                            'sMAPE (%)': metrics.get('sMAPE (%)', 'N/A')
                        }
                    
                    st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
                    results_df = pd.DataFrame(results).T.round(4)
                    st.dataframe(results_df, use_container_width=True)
                    
                    best_model_name = max(results.keys(), key=lambda x: results[x]['R¬≤'])
                    best_model = models_config[best_model_name]
                    best_model.fit(X_train, y_train)
                    
                    st.subheader(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
                    
                    fig = go.Figure()
                  
                    fig.add_trace(go.Scatter(
                        x=y_test,
                        y=best_model.predict(X_test),
                        mode='markers',
                        name='–ü—Ä–æ–≥–Ω–æ–∑—ã',
                        marker=dict(
                            color='#A7FC00',
                            size=6
                        ),
                        opacity=0.3
                    ))
                    
                    min_val = min(y_test.min(), best_model.predict(X_test).min())
                    max_val = max(y_test.max(), best_model.predict(X_test).max())
                    fig.add_trace(go.Scatter(
                        x=[min_val, max_val],
                        y=[min_val, max_val],
                        mode='lines',
                        name='–ò–¥–µ–∞–ª—å–Ω–æ',
                        line=dict(
                            dash='dash',
                            color='red',
                            width=3
                        ),
                        opacity=1.0
                    ))
                    
                    fig.update_layout(
                        title=f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - {selected_city}",
                        xaxis_title="–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                        yaxis_title="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            elif analysis_method == "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è":
                st.header("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
                
                target = st.selectbox(
                    "–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:",
                    numeric_cols[:10]
                )
                
                if target:
                    median_val = filtered_df[target].median()
                    st.write(f"**–ú–µ–¥–∏–∞–Ω–∞ {target}:** {median_val:.2f}")
                    st.write(f"**–ö–ª–∞—Å—Å—ã:** 0 = –Ω–∏–∂–µ –º–µ–¥–∏–∞–Ω—ã, 1 = –≤—ã—à–µ –º–µ–¥–∏–∞–Ω—ã")
                    
                    if len(numeric_cols) > 1:
                        correlations = filtered_df[numeric_cols].corr()[target].abs().sort_values(ascending=False)
                        correlations = correlations[correlations.index != target]
                        top_features = correlations.head(3).index.tolist()
                    else:
                        top_features = []
                    
                    features = st.multiselect(
                        "–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:",
                        numeric_cols,
                        default=top_features,
                        key="class_features"
                    )
                
                if target and features and len(features) > 0:
                    test_size = st.slider("–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏:", 0.1, 0.4, 0.3, 0.05, key="class_test_size")
                    
                    X, y, scaler, median_val = prepare_classification_data(filtered_df, target, features)
                    
                    if X is not None:
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=test_size, random_state=42, stratify=y
                        )
                      
                        models_config = {
                            "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è": LogisticRegression(random_state=42),
                            "Random Forest": RandomForestClassifier(n_estimators=50, random_state=42),
                            "Gradient Boosting": GradientBoostingClassifier(n_estimators=50, random_state=42),
                            "SVM": SVC(kernel='rbf', probability=True, random_state=42)
                        }
                        
                        results = {}
                        
                        for name, model in models_config.items():
                            model.fit(X_train, y_train)
                            y_pred = model.predict(X_test)
                            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
                            
                            report = classification_report(y_test, y_pred, output_dict=True)
                            results[name] = {
                                'Accuracy': accuracy_score(y_test, y_pred),
                                'Precision': report['weighted avg']['precision'],
                                'Recall': report['weighted avg']['recall'],
                                'F1-Score': report['weighted avg']['f1-score']
                            }
                        
                        st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                        results_df = pd.DataFrame(results).T.round(4)
                        st.dataframe(results_df, use_container_width=True)
                        
                        best_model_name = max(results.keys(), key=lambda x: results[x]['Accuracy'])
                        best_model = models_config[best_model_name]
                        best_model.fit(X_train, y_train)
                        
                        st.subheader(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
                        st.metric("Accuracy", f"{results[best_model_name]['Accuracy']:.4f}")
                        
                        from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
                        import matplotlib.pyplot as plt
                        
                        fig, ax = plt.subplots(figsize=(8, 6))
                        cm = confusion_matrix(y_test, best_model.predict(X_test))
                        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['–ù–∏–∂–µ –º–µ–¥–∏–∞–Ω—ã', '–í—ã—à–µ –º–µ–¥–∏–∞–Ω—ã'])
                        disp.plot(cmap='Blues', ax=ax)
                        ax.set_title(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {best_model_name}")
                        st.pyplot(fig)
                        
                        if hasattr(best_model, 'feature_importances_'):
                            st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                            importances = pd.DataFrame({
                                '–ü—Ä–∏–∑–Ω–∞–∫': features,
                                '–í–∞–∂–Ω–æ—Å—Ç—å': best_model.feature_importances_
                            }).sort_values('–í–∞–∂–Ω–æ—Å—Ç—å', ascending=False)
                            
                            fig = px.bar(
                                importances,
                                x='–í–∞–∂–Ω–æ—Å—Ç—å',
                                y='–ü—Ä–∏–∑–Ω–∞–∫',
                                orientation='h',
                                title='–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏'
                            )
                            st.plotly_chart(fig, use_container_width=True)
            
            elif analysis_method == "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è":
                st.header("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
                
                if len(numeric_cols) >= 2:
                    default_features = numeric_cols[:2]
                else:
                    default_features = numeric_cols
                
                features = st.multiselect(
                    "–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:",
                    numeric_cols[:6],
                    default=default_features
                )
                
                if len(features) >= 2:
                    algorithm = st.selectbox("–ê–ª–≥–æ—Ä–∏—Ç–º:", ["K-Means", "DBSCAN"])
                    
                    if algorithm == "K-Means":
                        n_clusters = st.slider("–ö–ª–∞—Å—Ç–µ—Ä–æ–≤:", 2, 6, 3)
                    else:
                        eps = st.slider("EPS:", 0.1, 1.0, 0.5, 0.1)
                    
                    X = df_scaled[features]
                    
                    sample_size = min(1000, len(X))
                    if len(X) > sample_size:
                        X_sample = X.sample(sample_size, random_state=42)
                    else:
                        X_sample = X
                    
                    if algorithm == "K-Means":
                        model = KMeans(n_clusters=n_clusters, n_init=3, random_state=42)
                        clusters = model.fit_predict(X_sample)
                        st.metric("Inertia", f"{model.inertia_:.2f}")
                        
                        # Silhouette score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                        if n_clusters > 1:
                            silhouette_avg = silhouette_score(X_sample, clusters)
                            st.metric("Silhouette Score", f"{silhouette_avg:.3f}")
                    else:
                        model = DBSCAN(eps=eps, min_samples=5)
                        clusters = model.fit_predict(X_sample)
                    
                    if len(X_sample) > 0:
                        df_viz = pd.DataFrame(X_sample, columns=features)
                        df_viz['Cluster'] = clusters
                        
                        try:
                            valid_indices = X_sample.index[X_sample.index.isin(filtered_df.index)]
                            if len(valid_indices) > 0:
                                for col in ['city_name', 'date']:
                                    if col in filtered_df.columns:
                                        df_viz[col] = filtered_df.loc[valid_indices, col].values
                        except:
                            pass
                    else:
                        df_viz = pd.DataFrame()
                    
                    if not df_viz.empty:
                        if features[0] in filtered_df.columns and pd.api.types.is_datetime64_any_dtype(filtered_df[features[0]]):
                            df_viz[features[0]] = convert_dates_to_numeric(df_viz[features[0]])
                        
                        fig = px.scatter(
                            df_viz,
                            x=features[0],
                            y=features[1],
                            color='Cluster',
                            title=f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {features[0]} vs {features[1]} - {selected_city}"
                        )
                        st.plotly_chart(fig, use_container_width=True)
            
            else:  # PCA
                st.header("–ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA)")
                
                if len(numeric_cols) >= 4:
                    pca_features = numeric_cols[:4]
                else:
                    pca_features = numeric_cols
                
                if len(pca_features) >= 2:
                    n_components = min(3, len(pca_features))
                    
                    X = df_scaled[pca_features]
                    
                    sample_size = min(1000, len(X))
                    if len(X) > sample_size:
                        X_sample = X.sample(sample_size, random_state=42)
                    else:
                        X_sample = X
                    
                    pca = PCA(n_components=n_components)
                    X_pca = pca.fit_transform(X_sample)
                    
                    explained_var = pca.explained_variance_ratio_
                    
                    fig = go.Figure()
                    fig.add_trace(go.Bar(
                        x=[f"PC{i+1}" for i in range(n_components)],
                        y=explained_var,
                        name='–î–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏'
                    ))
                    
                    fig.update_layout(
                        title=f"–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è - {selected_city}",
                        yaxis_title='–î–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    total_explained_var = sum(explained_var) * 100
                    st.metric("–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è", f"{total_explained_var:.1f}%")
                    
                    if n_components >= 2:
                        df_viz = pd.DataFrame(X_pca[:, :2], columns=['PC1', 'PC2'])
                        
                        try:
                            valid_indices = X_sample.index[X_sample.index.isin(filtered_df.index)]
                            if len(valid_indices) > 0:
                                for col in ['city_name', 'date']:
                                    if col in filtered_df.columns:
                                        df_viz[col] = filtered_df.loc[valid_indices, col].values
                        except:
                            pass
                        
                        fig_scatter = px.scatter(
                            df_viz,
                            x='PC1',
                            y='PC2',
                            color='city_name' if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞" and 'city_name' in df_viz.columns else None,
                            title=f"PCA - –ü—Ä–æ–µ–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö - {selected_city}"
                        )
                        st.plotly_chart(fig_scatter, use_container_width=True)

else:
    
    if filtered_df.empty:
        st.error("–î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ.")
    else:
        if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
            st.header(f"–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: –í—Å–µ –≥–æ—Ä–æ–¥–∞")
        else:
            st.header(f"–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: {selected_city}")
        
        if 'date' not in filtered_df.columns:
            st.error("–í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ —Å –¥–∞—Ç–∞–º–∏ (date)")
        else:
            numeric_cols = get_numeric_columns(filtered_df)
            
            if not numeric_cols:
                st.error("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è.")
            else:
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    target_col = st.selectbox(
                        "–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:",
                        numeric_cols[:10]
                    )
                
                with col2:
                    forecast_days = st.slider("–î–Ω–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:", 7, 90, 30)
                
                with col3:
                    if target_col:
                        data_accuracy = filtered_df[target_col].notna().sum() / len(filtered_df) * 100
                        st.metric("–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö", f"{data_accuracy:.1f}%")
                
                if selected_city == "–í—Å–µ –≥–æ—Ä–æ–¥–∞":
                    st.warning("–†–µ–∂–∏–º '–í—Å–µ –≥–æ—Ä–æ–¥–∞' - –¥–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω—ã")
                
                if target_col and target_col in filtered_df.columns:
                    with st.expander("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("–°—Ä–µ–¥–Ω–µ–µ", f"{filtered_df[target_col].mean():.2f}")
                        with col2:
                            st.metric("–ú–µ–¥–∏–∞–Ω–∞", f"{filtered_df[target_col].median():.2f}")
                        with col3:
                            st.metric("Std", f"{filtered_df[target_col].std():.2f}")
                
                if target_col:
                    with st.spinner("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞..."):
                        ts_data = prepare_time_series_data(filtered_df, target_col)
                    
                    if ts_data is not None:
                        st.subheader("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ä—è–¥–µ")
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("–î–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö", len(ts_data))
                        with col2:
                            start_date = ts_data['ds'].min()
                            st.metric("–ù–∞—á–∞–ª–æ", str(start_date.date()))
                        with col3:
                            end_date = ts_data['ds'].max()
                            st.metric("–ö–æ–Ω–µ—Ü", str(end_date.date()))
                        with col4:
                            ts_accuracy = ts_data['y'].notna().sum() / len(ts_data) * 100
                            st.metric("–¢–æ—á–Ω–æ—Å—Ç—å —Ä—è–¥–∞", f"{ts_accuracy:.1f}%")
                        
                        fig_original = px.line(
                            ts_data,
                            x='ds',
                            y='y',
                            title=f"–ò—Å—Ö–æ–¥–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥: {target_col} - {selected_city}",
                            line_shape='linear'
                        )
                        st.plotly_chart(fig_original, use_container_width=True)
                        
                        st.subheader("–ú–µ—Ç–æ–¥—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
                        
                        fast_mode = st.checkbox("–ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)", value=True)
                        
                        models_to_use = st.multiselect(
                            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:",
                            ["ARIMA", "Exponential Smoothing"],
                            default=["ARIMA", "Exponential Smoothing"]
                        )
                        
                        if models_to_use:
                            forecasts = {}
                            evaluation_results = {}
                            
                            for model_name in models_to_use:
                                with st.spinner(f"–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ {model_name}..."):
                                    metrics, test_data = evaluate_time_series_model(
                                        ts_data, 
                                        model_type='arima' if model_name == 'ARIMA' else 'exponential',
                                        test_size=0.3
                                    )
                                    
                                    if metrics:
                                        evaluation_results[model_name] = metrics
                                        
                                        if test_data:
                                            y_true, y_pred, dates = test_data
                                            
                                            fig_test = go.Figure()
                                            fig_test.add_trace(go.Scatter(
                                                x=dates,
                                                y=y_true,
                                                mode='lines',
                                                name='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è',
                                                line=dict(color='blue', width=2)
                                            ))
                                            fig_test.add_trace(go.Scatter(
                                                x=dates,
                                                y=y_pred,
                                                mode='lines',
                                                name=f'–ü—Ä–æ–≥–Ω–æ–∑ {model_name}',
                                                line=dict(color='red', width=2, dash='dash')
                                            ))
                                            
                                            fig_test.update_layout(
                                                title=f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_name} (30% –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö)",
                                                xaxis_title="–î–∞—Ç–∞",
                                                yaxis_title=target_col
                                            )
                                            
                                            with st.expander(f"–û—Ü–µ–Ω–∫–∞ {model_name}"):
                                                st.plotly_chart(fig_test, use_container_width=True)
                                                
                                                metrics_df = pd.DataFrame([metrics]).T
                                                metrics_df.columns = ['–ó–Ω–∞—á–µ–Ω–∏–µ']
                                                st.dataframe(metrics_df.round(4))
                                
                                with st.spinner(f"–ü—Ä–æ–≥–Ω–æ–∑ {model_name} –Ω–∞ –±—É–¥—É—â–µ–µ..."):
                                    if model_name == "ARIMA":
                                        _, forecast = arima_forecast(ts_data, periods=forecast_days)
                                    else:
                                        _, forecast = exponential_smoothing_forecast(ts_data, periods=forecast_days)
                                    
                                    forecasts[model_name] = forecast
                            
                            if evaluation_results:
                                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏")
                                
                                eval_df = pd.DataFrame(evaluation_results).T
                                
                                for col in eval_df.columns:
                                    if 'MAPE' in col or 'Improvement' in col:
                                        eval_df[col] = eval_df[col].apply(lambda x: f"{x:.2f}%" if not pd.isna(x) else "N/A")
                                    elif col == 'R¬≤':
                                        eval_df[col] = eval_df[col].apply(lambda x: f"{x:.4f}" if not pd.isna(x) else "N/A")
                                    else:
                                        eval_df[col] = eval_df[col].apply(lambda x: f"{x:.4f}" if not pd.isna(x) else "N/A")
                                
                                st.dataframe(eval_df, use_container_width=True)
                            
                            if forecasts:
                                st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
                                
                                fig_forecast = go.Figure()
                                
                                fig_forecast.add_trace(go.Scatter(
                                    x=ts_data['ds'],
                                    y=ts_data['y'],
                                    mode='lines',
                                    name='–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ',
                                    line=dict(color='#1f77b4', width=2)
                                ))
                                
                                colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
                                styles = ['solid', 'dash', 'dot', 'dashdot']
                                
                                for idx, (model_name, forecast_df) in enumerate(forecasts.items()):
                                    color = colors[idx % len(colors)]
                                    style = styles[idx % len(styles)]
                                    
                                    fig_forecast.add_trace(go.Scatter(
                                        x=forecast_df['ds'],
                                        y=forecast_df['yhat'],
                                        mode='lines',
                                        name=f'–ü—Ä–æ–≥–Ω–æ–∑ {model_name}',
                                        line=dict(color=color, width=2.5, dash=style)
                                    ))
                                
                                fig_forecast.update_layout(
                                    title=f"–ü—Ä–æ–≥–Ω–æ–∑ {target_col} –Ω–∞ {forecast_days} –¥–Ω–µ–π - {selected_city}",
                                    xaxis_title="–î–∞—Ç–∞",
                                    yaxis_title=target_col,
                                    hovermode='x unified'
                                )
                                
                                st.plotly_chart(fig_forecast, use_container_width=True)
                                
                                st.subheader("–ë—É–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è")
                                
                                forecast_table = pd.DataFrame()
                                for model_name, forecast_df in forecasts.items():
                                    temp_df = forecast_df.copy()
                                    temp_df.columns = ['–î–∞—Ç–∞', model_name]
                                    temp_df = temp_df.set_index('–î–∞—Ç–∞')
                                    
                                    if forecast_table.empty:
                                        forecast_table = temp_df
                                    else:
                                        forecast_table = forecast_table.join(temp_df, how='outer')
                                
                                if not forecast_table.empty:
                                    forecast_table = forecast_table.sort_index()
                                    st.dataframe(
                                        forecast_table.round(2),
                                        use_container_width=True
                                    )
                                    
                                    st.subheader("–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
                                    stats_data = []
                                    for model_name in forecasts.keys():
                                        values = forecast_table[model_name].dropna().values
                                        if len(values) > 0:
                                            stats_data.append([
                                                np.mean(values),
                                                np.std(values),
                                                np.min(values),
                                                np.max(values),
                                                (np.std(values) / max(abs(np.mean(values)), 0.001)) * 100
                                            ])
                                        else:
                                            stats_data.append(["N/A"] * 5)
                                    
                                    stats_df = pd.DataFrame(
                                        stats_data,
                                        index=forecasts.keys(),
                                        columns=['–°—Ä–µ–¥–Ω–µ–µ', '–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ', '–ú–∏–Ω–∏–º—É–º', '–ú–∞–∫—Å–∏–º—É–º', '–ö–æ—ç—Ñ. –≤–∞—Ä–∏–∞—Ü–∏–∏ (%)']
                                    )
                                    st.dataframe(stats_df.round(2), use_container_width=True)
